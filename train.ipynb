{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Input, Embedding, LSTM, Dense, merge, Convolution2D, MaxPooling2D, Reshape, Flatten, Dropout\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras import initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_file = '../train.p'\n",
    "testing_file = '../test.p'\n",
    "\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "\n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalize data\n",
    "X_train = X_train.astype(np.float32)/255\n",
    "X_train -= 0.5\n",
    "\n",
    "y_label = np.zeros(shape=(y_train.shape[0], np.max(y_train) + 1))\n",
    "\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_label[i, y_train[i]]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LavoeImageDataGenerator(ImageDataGenerator):\n",
    " \n",
    "    def random_transform(self, x):\n",
    "        # ImageGenerator from Keras was providing negative images, so I nverted the original input\n",
    "        img = 256 - x\n",
    "        \n",
    "        ang_range = 2\n",
    "        shear_range = 2\n",
    "        trans_range = 2\n",
    "        brightness = False\n",
    "\n",
    "        # Rotation\n",
    "        ang_rot = np.random.uniform(ang_range)-ang_range/2\n",
    "        rows,cols,ch = img.shape    \n",
    "        Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)\n",
    "\n",
    "        # Translation\n",
    "        tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "        tr_y = trans_range*np.random.uniform()-trans_range/2\n",
    "        Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "\n",
    "        # Shear\n",
    "        pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "\n",
    "        pt1 = 5+shear_range*np.random.uniform()-shear_range/2\n",
    "        pt2 = 20+shear_range*np.random.uniform()-shear_range/2\n",
    "\n",
    "        # Brightness \n",
    "        pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
    "\n",
    "        shear_M = cv2.getAffineTransform(pts1,pts2)\n",
    "\n",
    "        img = cv2.warpAffine(img,Rot_M,(cols,rows))\n",
    "        img = cv2.warpAffine(img,Trans_M,(cols,rows))\n",
    "        img = cv2.warpAffine(img,shear_M,(cols,rows))\n",
    "\n",
    "\n",
    "        if brightness:\n",
    "            img = cv2.cvtColor(img,cv2.COLOR_RGB2HSV)\n",
    "            random_bright = .25+np.random.uniform()\n",
    "            #print(random_bright)\n",
    "            img[:,:,2] = img[:,:,2]*random_bright\n",
    "            img = cv2.cvtColor(img,cv2.COLOR_HSV2RGB)\n",
    "\n",
    "        # Make sure the output is from 0 - 255.\n",
    "        img = img.astype(np.uint8)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model saving setup and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_init(shape, name=None):\n",
    "    return initializations.normal(shape, scale=0.01, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final Tensor(\"Softmax:0\", shape=(?, 43), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# input layer\n",
    "main_input = Input(shape=(32, 32, 3), name='main_input')\n",
    "\n",
    "color_map = Convolution2D(3, 1, 1, init=my_init, border_mode='same')(main_input)\n",
    "\n",
    "conv1 = Convolution2D(32, 3, 3, init=my_init, border_mode='same')(color_map)\n",
    "conv2 = Convolution2D(32, 3, 3, init=my_init, border_mode='same')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(3, 3))(conv2)\n",
    "drop1 = Dropout(0.5)(pool1)\n",
    "\n",
    "conv3 = Convolution2D(64, 3, 3, init=my_init, border_mode='same')(drop1)\n",
    "conv4 = Convolution2D(64, 3, 3, init=my_init, border_mode='same')(conv3)\n",
    "pool2 = MaxPooling2D(pool_size=(3, 3))(conv4)\n",
    "drop2 = Dropout(0.5)(pool2)\n",
    "\n",
    "conv5 = Convolution2D(128, 3, 3, init=my_init, border_mode='same')(drop2)\n",
    "conv6 = Convolution2D(128, 3, 3, init=my_init, border_mode='same')(conv5)\n",
    "pool3 = MaxPooling2D(pool_size=(3, 3))(conv6)\n",
    "drop3 = Dropout(0.5)(pool3)\n",
    "\n",
    "# flatten layer\n",
    "flat1 = Flatten()(drop1)\n",
    "flat2 = Flatten()(drop2)\n",
    "flat3 = Flatten()(drop3)\n",
    "\n",
    "# concatenate all vectors\n",
    "merged1 = merge([flat1, flat2, flat3], name='merge1', mode='concat',concat_axis = 1)\n",
    "\n",
    "fc1 = Dense(1024)(merged1)\n",
    "drop4 = Dropout(0.5)(fc1)\n",
    "\n",
    "fc2 = Dense(1024)(drop4)\n",
    "drop5 = Dropout(0.5)(fc2)\n",
    "\n",
    "softmax_layer = Dense(43)(drop5)\n",
    "main_output = Activation('softmax')(softmax_layer)\n",
    "print(\"final\", main_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = LavoeImageDataGenerator(width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             rotation_range=20,\n",
    "                             zoom_range=0.2,\n",
    "                             shear_range=0,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import merge\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def make_parallel(model, gpu_count):\n",
    "    def get_slice(data, idx, parts):\n",
    "        shape = tf.shape(data)\n",
    "        size = tf.concat(0, [ shape[:1] // parts, shape[1:] ])\n",
    "        stride = tf.concat(0, [ shape[:1] // parts, shape[1:]*0 ])\n",
    "        start = stride * idx\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    outputs_all = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        outputs_all.append([])\n",
    "\n",
    "    #Place a copy of the model on each GPU, each getting a slice of the batch\n",
    "    for i in range(gpu_count):\n",
    "        with tf.device('/gpu:%d' % i):\n",
    "            with tf.name_scope('tower_%d' % i) as scope:\n",
    "\n",
    "                inputs = []\n",
    "                #Slice each input into a piece for processing on this GPU\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_n = Lambda(get_slice, output_shape=input_shape, arguments={'idx':i,'parts':gpu_count})(x)\n",
    "                    inputs.append(slice_n)                \n",
    "\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "                \n",
    "                #Save all the outputs for merging back together later\n",
    "                for l in range(len(outputs)):\n",
    "                    outputs_all[l].append(outputs[l])\n",
    "\n",
    "    # merge outputs on CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for outputs in outputs_all:\n",
    "            merged.append(merge(outputs, mode='concat', concat_axis=0))\n",
    "            \n",
    "        return Model(input=model.inputs, output=merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gpu_count = 16\n",
    "batch_size = 2048\n",
    "prefix = 'traffic'\n",
    "model_path ='.model_{0}.h5'.format(prefix)\n",
    "epochs = 1000\n",
    "learning_rate = 0.001\n",
    "decay_rate = learning_rate / epochs\n",
    "sgd_select = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=decay_rate)\n",
    "\n",
    "dataset_size = gpu_count * batch_size\n",
    "\n",
    "sign_model = Model(input = [main_input], output = [main_output])\n",
    "\n",
    "sign_model = make_parallel(sign_model, gpu_count)\n",
    "\n",
    "sign_model.compile(optimizer = sgd_select, loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "32768/32768 [==============================] - 27s - loss: 3.7628 - acc: 0.0212\n",
      "Epoch 2/1000\n",
      "32768/32768 [==============================] - 1s - loss: 3.6829 - acc: 0.0826\n",
      "Epoch 3/1000\n",
      "32768/32768 [==============================] - 1s - loss: 3.4368 - acc: 0.0753\n",
      "Epoch 4/1000\n",
      "32768/32768 [==============================] - 1s - loss: 3.7753 - acc: 0.0800\n",
      "Epoch 5/1000\n",
      "32768/32768 [==============================] - 1s - loss: 3.3803 - acc: 0.0828\n",
      "Epoch 6/1000\n",
      "32768/32768 [==============================] - 1s - loss: 3.4286 - acc: 0.0981\n",
      "Epoch 7/1000\n",
      "32768/32768 [==============================] - 1s - loss: 3.4698 - acc: 0.0971\n",
      "Epoch 8/1000\n",
      "32768/32768 [==============================] - 1s - loss: 3.4461 - acc: 0.0934\n",
      "Epoch 9/1000\n",
      "32768/32768 [==============================] - 2s - loss: 3.3935 - acc: 0.0859\n",
      "Epoch 10/1000\n",
      "32768/32768 [==============================] - 3s - loss: 3.3500 - acc: 0.0819\n",
      "Epoch 11/1000\n",
      "32768/32768 [==============================] - 3s - loss: 3.2811 - acc: 0.0850\n",
      "Epoch 12/1000\n",
      "32768/32768 [==============================] - 3s - loss: 3.1787 - acc: 0.0996\n",
      "Epoch 13/1000\n",
      "32768/32768 [==============================] - 3s - loss: 3.0972 - acc: 0.1461\n",
      "Epoch 14/1000\n",
      "32768/32768 [==============================] - 3s - loss: 3.0621 - acc: 0.1410\n",
      "Epoch 15/1000\n",
      "32768/32768 [==============================] - 3s - loss: 3.0409 - acc: 0.1376\n",
      "Epoch 16/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.9856 - acc: 0.1705\n",
      "Epoch 17/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.9207 - acc: 0.1946\n",
      "Epoch 18/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.8632 - acc: 0.2047\n",
      "Epoch 19/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.8100 - acc: 0.2121\n",
      "Epoch 20/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.7674 - acc: 0.2174\n",
      "Epoch 21/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.7246 - acc: 0.2229\n",
      "Epoch 22/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.6940 - acc: 0.2314\n",
      "Epoch 23/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.6758 - acc: 0.2344\n",
      "Epoch 24/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.6426 - acc: 0.2372\n",
      "Epoch 25/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.6175 - acc: 0.2447\n",
      "Epoch 26/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.5872 - acc: 0.2550\n",
      "Epoch 27/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.5549 - acc: 0.2682\n",
      "Epoch 28/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.5280 - acc: 0.2725\n",
      "Epoch 29/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.4969 - acc: 0.2811\n",
      "Epoch 30/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.4780 - acc: 0.2870\n",
      "Epoch 31/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.4565 - acc: 0.2919\n",
      "Epoch 32/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.4274 - acc: 0.2966\n",
      "Epoch 33/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.4042 - acc: 0.3011\n",
      "Epoch 34/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.3741 - acc: 0.3094\n",
      "Epoch 35/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.3517 - acc: 0.3133\n",
      "Epoch 36/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.3224 - acc: 0.3179\n",
      "Epoch 37/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.2955 - acc: 0.3278\n",
      "Epoch 38/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.2739 - acc: 0.3359\n",
      "Epoch 39/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.2556 - acc: 0.3386\n",
      "Epoch 40/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.2383 - acc: 0.3447\n",
      "Epoch 41/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.2179 - acc: 0.3493\n",
      "Epoch 42/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.1934 - acc: 0.3556\n",
      "Epoch 43/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.1728 - acc: 0.3648\n",
      "Epoch 44/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.1541 - acc: 0.3652\n",
      "Epoch 45/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.1429 - acc: 0.3692\n",
      "Epoch 46/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.1205 - acc: 0.3768\n",
      "Epoch 47/1000\n",
      "32768/32768 [==============================] - 4s - loss: 2.0981 - acc: 0.3803\n",
      "Epoch 48/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.0808 - acc: 0.3870\n",
      "Epoch 49/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.0637 - acc: 0.3931\n",
      "Epoch 50/1000\n",
      "32768/32768 [==============================] - 4s - loss: 2.0497 - acc: 0.3953\n",
      "Epoch 51/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.0362 - acc: 0.3961\n",
      "Epoch 52/1000\n",
      "32768/32768 [==============================] - 4s - loss: 2.0202 - acc: 0.4040\n",
      "Epoch 53/1000\n",
      "32768/32768 [==============================] - 3s - loss: 2.0091 - acc: 0.4091\n",
      "Epoch 54/1000\n",
      "32768/32768 [==============================] - 4s - loss: 1.9863 - acc: 0.4124\n",
      "Epoch 55/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.9742 - acc: 0.4200\n",
      "Epoch 56/1000\n",
      "32768/32768 [==============================] - 4s - loss: 1.9524 - acc: 0.4240\n",
      "Epoch 57/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.9495 - acc: 0.4232\n",
      "Epoch 58/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.9444 - acc: 0.4262\n",
      "Epoch 59/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.9248 - acc: 0.4298\n",
      "Epoch 60/1000\n",
      "32768/32768 [==============================] - 4s - loss: 1.9129 - acc: 0.4370\n",
      "Epoch 61/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.9140 - acc: 0.4317\n",
      "Epoch 62/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.9070 - acc: 0.4352\n",
      "Epoch 63/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.8974 - acc: 0.4388\n",
      "Epoch 64/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.8679 - acc: 0.4464\n",
      "Epoch 65/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.8810 - acc: 0.4403\n",
      "Epoch 66/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.8557 - acc: 0.4492\n",
      "Epoch 67/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.8533 - acc: 0.4522\n",
      "Epoch 68/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.8349 - acc: 0.4569\n",
      "Epoch 69/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.8289 - acc: 0.4580\n",
      "Epoch 70/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.8215 - acc: 0.4597\n",
      "Epoch 71/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.8136 - acc: 0.4655\n",
      "Epoch 72/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.8064 - acc: 0.4633\n",
      "Epoch 73/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.8062 - acc: 0.4637\n",
      "Epoch 74/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7896 - acc: 0.4692\n",
      "Epoch 75/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7924 - acc: 0.4706\n",
      "Epoch 76/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7752 - acc: 0.4727\n",
      "Epoch 77/1000\n",
      "32768/32768 [==============================] - 4s - loss: 1.7724 - acc: 0.4747\n",
      "Epoch 78/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7666 - acc: 0.4770\n",
      "Epoch 79/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7553 - acc: 0.4798\n",
      "Epoch 80/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7595 - acc: 0.4769\n",
      "Epoch 81/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7538 - acc: 0.4831\n",
      "Epoch 82/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7443 - acc: 0.4837\n",
      "Epoch 83/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7328 - acc: 0.4872\n",
      "Epoch 84/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7298 - acc: 0.4905\n",
      "Epoch 85/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7181 - acc: 0.4884\n",
      "Epoch 86/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7229 - acc: 0.4928\n",
      "Epoch 87/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7164 - acc: 0.4932\n",
      "Epoch 88/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7200 - acc: 0.4902\n",
      "Epoch 89/1000\n",
      "32768/32768 [==============================] - 4s - loss: 1.7067 - acc: 0.4935\n",
      "Epoch 90/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7013 - acc: 0.4956\n",
      "Epoch 91/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.7008 - acc: 0.4930\n",
      "Epoch 92/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6932 - acc: 0.4971\n",
      "Epoch 93/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6794 - acc: 0.5009\n",
      "Epoch 94/1000\n",
      "32768/32768 [==============================] - 4s - loss: 1.6864 - acc: 0.4993\n",
      "Epoch 95/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6812 - acc: 0.5016\n",
      "Epoch 96/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6777 - acc: 0.5011\n",
      "Epoch 97/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6736 - acc: 0.5065\n",
      "Epoch 98/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6679 - acc: 0.5061\n",
      "Epoch 99/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6608 - acc: 0.5061\n",
      "Epoch 100/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6666 - acc: 0.5058\n",
      "Epoch 101/1000\n",
      "32768/32768 [==============================] - 4s - loss: 1.6551 - acc: 0.5104\n",
      "Epoch 102/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6713 - acc: 0.5032\n",
      "Epoch 103/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6488 - acc: 0.5106\n",
      "Epoch 104/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6473 - acc: 0.5156\n",
      "Epoch 105/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6474 - acc: 0.5109\n",
      "Epoch 106/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6321 - acc: 0.5174\n",
      "Epoch 107/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6350 - acc: 0.5162\n",
      "Epoch 108/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6335 - acc: 0.5131\n",
      "Epoch 109/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6232 - acc: 0.5180\n",
      "Epoch 110/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6204 - acc: 0.5216\n",
      "Epoch 111/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6220 - acc: 0.5199\n",
      "Epoch 112/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6168 - acc: 0.5222\n",
      "Epoch 113/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6170 - acc: 0.5217\n",
      "Epoch 114/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6170 - acc: 0.5213\n",
      "Epoch 115/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6122 - acc: 0.5212\n",
      "Epoch 116/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6019 - acc: 0.5239\n",
      "Epoch 117/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6103 - acc: 0.5243\n",
      "Epoch 118/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6026 - acc: 0.5264\n",
      "Epoch 119/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5954 - acc: 0.5260\n",
      "Epoch 120/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.6037 - acc: 0.5256\n",
      "Epoch 121/1000\n",
      "32768/32768 [==============================] - 4s - loss: 1.5952 - acc: 0.5281\n",
      "Epoch 122/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5869 - acc: 0.5291\n",
      "Epoch 123/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5864 - acc: 0.5277\n",
      "Epoch 124/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5898 - acc: 0.5305\n",
      "Epoch 125/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5936 - acc: 0.5279\n",
      "Epoch 126/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5908 - acc: 0.5250\n",
      "Epoch 127/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5880 - acc: 0.5312\n",
      "Epoch 128/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5737 - acc: 0.5320\n",
      "Epoch 129/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5771 - acc: 0.5300\n",
      "Epoch 130/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5723 - acc: 0.5353\n",
      "Epoch 131/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5703 - acc: 0.5331\n",
      "Epoch 132/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5710 - acc: 0.5332\n",
      "Epoch 133/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5665 - acc: 0.5345\n",
      "Epoch 134/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5578 - acc: 0.5377\n",
      "Epoch 135/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5637 - acc: 0.5373\n",
      "Epoch 136/1000\n",
      "32768/32768 [==============================] - 3s - loss: 1.5657 - acc: 0.5371\n",
      "Epoch 137/1000\n"
     ]
    }
   ],
   "source": [
    "sign_model.fit_generator(datagen.flow(X_train[:dataset_size], y_label[:dataset_size],\n",
    "                                      shuffle=True, batch_size=gpu_count * batch_size),\n",
    "                    samples_per_epoch=dataset_size, nb_epoch=epochs)\n",
    "sign_model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "sign_model.load_weights(model_path)\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
